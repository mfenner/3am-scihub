---
title: "3 am sci hub"
output: md_document
---

```{r}
knitr::opts_chunk$set(
  fig.width=10,
 message=FALSE,
 warning=FALSE,
 fig.path='figure/',
 fig.cap = "", 
 echo = TRUE,
 warning = FALSE, 
 message = FALSE
)
```


Install the required packages:

```{r loadpkgs}
options(stringsAsFactors = FALSE)

# install required packages
# install.packages("tidyverse")
# install.packages("rcrossref")
library('tidyverse')
library('rcrossref')
```

## Load Sci-Hub Data into R

The SciHub paper:

Bohannon J (2016) Who's downloading pirated papers? Everyone. Science 352(6285): 508-512. [https://doi.org/10.1126/science.352.6285.508](https://doi.org/10.1126/science.352.6285.508)

The associated dataset:

Elbakyan A, Bohannon J (2016) Data from: Who's downloading pirated papers? Everyone. Dryad Digital Repository. [https://doi.org/10.5061/dryad.q447c](https://doi.org/10.5061/dryad.q447c)

To load the dataset, download the file from the Dryad landing page, or directly from [here](http://datadryad.org/bitstream/handle/10255/dryad.114259/scihub_data.zip?sequence=1).

After downloading the data, copy the folder `scihub_data` into a new `data` folder:

To load one file representing one month, simply type:

```{r}
my_data <- readr::read_tsv(file = "data/scihub_data/dec2015.tab", col_names = FALSE)
```

Now let's inspect the data:

```{r}
my_data
```

Is it clean?

```{r}
library(dplyr)
my_data %>% 
  dplyr::group_by(X4) %>% 
  dplyr::summarise(Counts = n()) %>%
  dplyr::arrange(desc(Counts))
```

It looks clean, great!

Well, the data is huge and you might run out of memory if you try to load the whole dataset. So, let's subset our data and save it to your disk. Let's define a function:

```{r, echo=TRUE}
my_helper <- function(file = NULL) {
  tt <- readr::read_tsv(file = file, col_names = FALSE) %>%
  dplyr::filter(X4 == "Iran")
  file_name <- gsub(".tab", "_iran.csv", file)
  write.csv(tt,  file_name, row.names = FALSE)
}
```

Get files

```{r}
my_files <- list.files("data/scihub_data/", pattern = "tab")
my_files <- paste0("data/scihub_data/", my_files)
```

And apply it

```{r}
sapply(my_files, my_helper)
```

We now have the subset on our local disk. Let's load in the whole Iranian usage events.


```{r}
my_files <- list.files("data/scihub_data/", pattern = "iran")
my_files <- paste0("data/scihub_data/", my_files)
```

```{r}
sci_hub_ir <- NULL

for (i in my_files) {
  my_data <- readr::read_csv(file = i)
  sci_hub_ir <- rbind(sci_hub_ir, my_data)
}
```

So, let's inspect the data frame and save a local copy

```{r}
sci_hub_ir
write.csv(sci_hub_ir, "data/iran.csv", row.names = FALSE)
```

## Analysis

Possible questions:

What is the usage by:
- month
- city
- subject (defined by publishers at the journal level, might be a bit messy)
- journal
- publisher

To get subject, journal and publisher information, we need to use data from Crossref based on the DOI.

### By month


## Fetching data from Crossref with the `rcrossref` package

We could use [rcrossref](https://github.com/ropensci/rcrossref) to fetch information on:

- subject (defined by publishers at the journal level, might be a bit messy)
- journal
- publisher

Make sure to install `rcrossref` from CRAN

```
install.packages("rcrossref")
library(rcrossref)
```

And now get metadata for a sample of 10 publications

```{r}
my_dois <- sample(sci_hub_ir$X2, 10)
```

And now fetch the metadata

```{r}
my_md <- rcrossref::cr_works(my_dois)
```

And let's inspect it:

```{r}
my_md$data
```

Wow, very comprehensive!

### By date

First step: Transform to dates

```{r}
sci_hub_ir <- sci_hub_ir %>% 
  dplyr::mutate(date = format(sci_hub_ir$X1, "%Y-%m-%d"))
```

Second step: Create summary of events per day

```{r}
ir_per_date <- sci_hub_ir %>% 
  group_by(date) %>% 
  summarize(events = n())
ir_per_date
```

Well, we know that there are missing values in November. Let's deal with it

```{r}
# make a date sequence as data.frame
my_dates <- seq(as.Date("2015-09-01"), as.Date("2016-02-28"), by = "day")
my_dates <- as.data.frame(my_dates)
# merge it
ir_per_date$date <- as.Date(ir_per_date$date)
ir_t <- right_join(ir_per_date, my_dates, by = c("date" = "my_dates"))
# deal with empty values  
ir_t$events[is.na(ir_t$events)] <- 0
```

Let's plot:

```{r, fig.height= 4, fig.width= 12}
ggplot(ir_t, aes(as.Date(date), events, group = 1)) + 
  geom_area(fill = "#740e18") + 
  scale_x_date(date_breaks = "1 month", date_labels = "%m") + 
  xlab("Date") +
  ylab("Sci-Hub Downloads from Iran") +
  theme_bw()
```

Show the top 10 busiest days as well.

```{r}
ir_per_date %>% arrange(desc(events))
```

Wow, usage in Iran peaked around christmas. Why?

